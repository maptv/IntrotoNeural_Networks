{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b28a8c0",
   "metadata": {},
   "source": [
    "## INTROTONEURALNETWORKS/3 INTROTOTENSORFLOW/INTROTONEURALNETWORKS INTROTOTENSORFLOW 2 EXERCISE ANSWERS ##\n",
    "#### Exercise ####\n",
    "#### Please refer to module 2 of IntroToNeuralNetworks - IntroToTensorflow for Tasks 1-7\n",
    "#### Task 1 \n",
    "##### Load the libraries that are used in this module.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d27715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper packages.\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn packages.\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# TensorFlow and supporting packages.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba560afb",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Define the data directory.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f566b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "home_dir = Path(\".\").resolve()\n",
    "main_dir = home_dir.parent.parent\n",
    "print(main_dir)\n",
    "data_dir = str(main_dir) + \"/data\"\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae93e3a2",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Load the dataset `bank_marketing.csv` and save it to `bank_marketing`.\n",
    "##### Print the first few rows of `bank_marketing`.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c66b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_marketing = pd.read_csv(data_dir + \"/bank_marketing.csv\")\n",
    "bank_marketing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4537d22",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Define a convenience function `ex_data_prep` to perform the data cleaning steps mentioned below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c223663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Replace the column `y` in the dataframe, by setting it to 1 if `y` is 'yes', otherwise set `y` to 0.\n",
    "2. Perform one hot encoding on the variables with data type object (i.e `job`, `marital`, `education`, `default`, `housing`, `loan`, `contact`, `month`, `day_of_week` and `poutcome`) except the target variable `y`\n",
    "3. Drop the original variables and concatenate the dummies to original datset\n",
    "4. Select the predictors by dropping variable `y` and save the result to a dataframe `X_ex`\n",
    "5. Save the target variable `y` column to `y_ex` variable\n",
    "6. Set the seed to 1\n",
    "7. Split the data into training, test, and validation sets with 70:15:15 ratio and save respective variables to `X_train_ex`, `X_test_ex`, `X_val_ex`, `y_train_ex`, `y_test_ex`, `y_val_ex`\n",
    "8. Scale the train, test and the validation datasets using Min max scaler and save as `X_train_scaled_ex`, `X_test_scaled_ex` and `X_val_scaled_ex` respectiely\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c844abc",
   "metadata": {},
   "source": [
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e16ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex_data_prep(df):\n",
    "\n",
    "    # Convert `y` to 0/1 values\n",
    "    df[\"y\"] = np.where(df[\"y\"] == \"yes\", 1, 0)\n",
    "\n",
    "    # Perform one hot encoding\n",
    "    job_dummy = pd.get_dummies(df[\"job\"], prefix=\"job\", drop_first=True)\n",
    "    marital_dummy = pd.get_dummies(df[\"marital\"], prefix=\"marital\", drop_first=True)\n",
    "    education_dummy = pd.get_dummies(\n",
    "        df[\"education\"], prefix=\"education\", drop_first=True\n",
    "    )\n",
    "    default_dummy = pd.get_dummies(df[\"default\"], prefix=\"default\", drop_first=True)\n",
    "    housing_dummy = pd.get_dummies(df[\"housing\"], prefix=\"housing\", drop_first=True)\n",
    "    loan_dummy = pd.get_dummies(df[\"loan\"], prefix=\"loan\", drop_first=True)\n",
    "    contact_dummy = pd.get_dummies(df[\"contact\"], prefix=\"contact\", drop_first=True)\n",
    "    month_dummy = pd.get_dummies(df[\"month\"], prefix=\"month\", drop_first=True)\n",
    "    day_of_week_dummy = pd.get_dummies(\n",
    "        df[\"day_of_week\"], prefix=\"day_of_week\", drop_first=True\n",
    "    )\n",
    "    poutcome_dummy = pd.get_dummies(df[\"poutcome\"], prefix=\"poutcome\", drop_first=True)\n",
    "\n",
    "    # Drop the original variables\n",
    "    df.drop(\n",
    "        [\n",
    "            \"job\",\n",
    "            \"marital\",\n",
    "            \"education\",\n",
    "            \"default\",\n",
    "            \"housing\",\n",
    "            \"loan\",\n",
    "            \"contact\",\n",
    "            \"month\",\n",
    "            \"day_of_week\",\n",
    "            \"poutcome\",\n",
    "        ],\n",
    "        axis=1,\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # Concatenate the dummies to original dataset\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            df,\n",
    "            job_dummy,\n",
    "            marital_dummy,\n",
    "            education_dummy,\n",
    "            default_dummy,\n",
    "            housing_dummy,\n",
    "            loan_dummy,\n",
    "            contact_dummy,\n",
    "            month_dummy,\n",
    "            day_of_week_dummy,\n",
    "            poutcome_dummy,\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Separate predictors from target variable.\n",
    "    X_ex = df.drop([\"y\"], axis=1)\n",
    "    y_ex = df[\"y\"]\n",
    "\n",
    "    # Set the seed to 1.\n",
    "    np.random.seed(1)\n",
    "    # Split data into train, test, and validation set, use a 70 - 15 - 15 split.\n",
    "    # First split data into train-test with 70% for train and 30% for test.\n",
    "    X_train_ex, X_test_ex, y_train_ex, y_test_ex = train_test_split(\n",
    "        X_ex.values, y_ex, test_size=0.3, random_state=1\n",
    "    )\n",
    "    # Then split the test data into two halves: test and validation.\n",
    "    X_test_ex, X_val_ex, y_test_ex, y_val_ex = train_test_split(\n",
    "        X_test_ex, y_test_ex, test_size=0.5, random_state=1\n",
    "    )\n",
    "    print(\n",
    "        \"Train shape:\",\n",
    "        X_train_ex.shape,\n",
    "        \"Test shape:\",\n",
    "        X_test_ex.shape,\n",
    "        \"Val shape:\",\n",
    "        X_val_ex.shape,\n",
    "    )\n",
    "\n",
    "    # Transforms features by scaling each feature to a given range.\n",
    "    # The default is the range between 0 and 1.\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    X_train_scaled_ex = min_max_scaler.fit_transform(X_train_ex)\n",
    "    X_test_scaled_ex = min_max_scaler.transform(X_test_ex)\n",
    "    X_val_scaled_ex = min_max_scaler.transform(X_val_ex)\n",
    "\n",
    "    return (\n",
    "        X_train_scaled_ex,\n",
    "        X_test_scaled_ex,\n",
    "        X_val_scaled_ex,\n",
    "        y_train_ex,\n",
    "        y_test_ex,\n",
    "        y_val_ex,\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    "    X_train_scaled_ex,\n",
    "    X_test_scaled_ex,\n",
    "    X_val_scaled_ex,\n",
    "    y_train_ex,\n",
    "    y_test_ex,\n",
    "    y_val_ex,\n",
    ") = ex_data_prep(bank_marketing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c84534",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Initialize a simple sequential neural network model with 32 neurons for the 1st hidden layer, 20 neurons for the second layer, and appropriate input and output layers. Name the model `model_ex`.\n",
    "##### Compile the model using the \"adam\" optimizer and \"binary_crossentropy\" loss, and use \"accuracy\" as a metric.\n",
    "##### Fit the model using train and validation sets with 200 epochs, and assign it to `model_res_ex` variable.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568c50eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set the seed so that we can reproduce the results.\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238297e1",
   "metadata": {},
   "source": [
    "## Set up model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ab7305",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ex = Sequential(\n",
    "    [\n",
    "        Dense(\n",
    "            32,  # <- number of neurons for 1st hidden layer\n",
    "            input_shape=(X_train_scaled_ex.shape[1],),\n",
    "        ),  # <- input layer shape: `(num_features, )`\n",
    "        Dense(\n",
    "            20, activation=\"relu\"  # <- add 2nd hidden layer with 20 neurons\n",
    "        ),  # <- set activation function for hidden layer\n",
    "        Dense(\n",
    "            1, activation=\"sigmoid\"  # <- add output layer with single neuron\n",
    "        ),  # <- activation function for output layer\n",
    "    ]\n",
    ")\n",
    "# Compile the model.\n",
    "model_ex.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Fitting the model\n",
    "model_res_ex = model_ex.fit(\n",
    "    X_train_scaled_ex,\n",
    "    y_train_ex,  # <- train data and labels\n",
    "    validation_data=(X_val_scaled_ex, y_val_ex),  # <- pass validation data\n",
    "    epochs=200,\n",
    ")  # <- specify number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d660f61",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Plot accuracy and loss curves for `model_res_ex`. \n",
    "##### You can access them by checking the `model_res_ex.history` dictionary.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432de772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(model_res_ex.history[\"accuracy\"])  # <- accuracy scores\n",
    "plt.plot(\n",
    "    model_res_ex.history[\"val_accuracy\"]\n",
    ")  # <- get validation accuracy scores from dictionary\n",
    "plt.title(\"Model accuracy\")  # <- adding title to the plot\n",
    "plt.ylabel(\"Accuracy\")  # <- naming the y-axis\n",
    "plt.xlabel(\"Epoch\")  # <- naming the x-axis\n",
    "plt.legend([\"Train\", \"Val\"], loc=\"upper left\")  # <- adding a legend\n",
    "plt.show()  # <- displaying the plot\n",
    "# Plot training & validation loss values\n",
    "plt.plot(model_res_ex.history[\"loss\"])  # <- loss values\n",
    "plt.plot(\n",
    "    model_res_ex.history[\"val_loss\"]\n",
    ")  # <- get validation loss scores from dictionary\n",
    "plt.title(\"Model loss\")  # <- adding title to the plot\n",
    "plt.ylabel(\"Loss\")  # <- naming the y-axis\n",
    "plt.xlabel(\"Epoch\")  # <- naming the x-axis\n",
    "plt.legend([\"Train\", \"Val\"], loc=\"upper left\")  # <- adding a legend\n",
    "plt.show()  # <- displaying the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a53f0",
   "metadata": {},
   "source": [
    "#### Task 7\n",
    "##### Evaluate the model on the test data. Save loss as `loss_ex` and accuracy as `accuracy_ex`. Print the loss and accuracy values.\n",
    "##### Predict the labels on test data for the model and save as `y_pred_ex`. \n",
    "##### Check how the values look.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e832f962",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ex, accuracy_ex = model_ex.evaluate(x=X_test_scaled_ex, y=y_test_ex)\n",
    "print(\"Loss: {0:6.3f}, Accuracy: {1:6.3f}\".format(loss_ex, accuracy_ex))\n",
    "y_pred_ex = (model_ex.predict(X_test_scaled_ex) > 0.5).astype(\"int32\")\n",
    "print(y_pred_ex)"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
